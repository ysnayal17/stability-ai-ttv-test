{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysnayal17/stability-ai-ttv-test/blob/main/Generate_Images_From_Text_With_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n"
      ],
      "metadata": {
        "id": "JVaEGdE5Uz_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate An Image From A Text Prompt With Stable Diffusion\n",
        "\n",
        "It needs to be noted that this is a copy of the Colab from Stability AI [here](https://colab.research.google.com/github/woctezuma/stable-diffusion-colab/blob/main/stable_diffusion.ipynb).  All I did was add instructions/explanations and the ability to add the text prompt in a field instead of into the code.\n",
        "\n",
        "Let's get going."
      ],
      "metadata": {
        "id": "MG5g0M4IOONW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First** we need to update some of the preinstalled libraries:"
      ],
      "metadata": {
        "id": "lW7e7SGgOwBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuFz5uGi-h6G"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade diffusers transformers scipy mediapy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to log into Hugging Face.\n",
        "\n",
        "Hugging Face hosts the model for Stability AI, which is why the account is needed.\n",
        "\n",
        "When you run the cell you'll be presented with a link to head over to get your token. After you've either logged in or created an account, you'll be given a token to paste into the field created in the cell."
      ],
      "metadata": {
        "id": "vGfDKqxoPLSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "GR4vF2bw-sHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next we just need to import the model and required files."
      ],
      "metadata": {
        "id": "2ZNFvlHzQFDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import PNDMScheduler, DDIMScheduler, LMSDiscreteScheduler\n",
        "\n",
        "scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True)\n",
        "# scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "# scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")"
      ],
      "metadata": {
        "id": "vF9Q0xKX8gLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapy as media\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "device = \"cuda\"\n",
        "remove_safety = False\n",
        "\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16, revision=\"fp16\", use_auth_token=True)\n",
        "if remove_safety:\n",
        "  pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "pipe = pipe.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "bG2hkmSEvByV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#And last, just enter your text prompt:"
      ],
      "metadata": {
        "id": "nmJ4W1GVQYbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = input('Enter the text prompt you want used to generate the image. The more detailed, the more fun. :) ')"
      ],
      "metadata": {
        "id": "zQkEKND_QuI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#And click the play button to generate your image!\n",
        "\n",
        "And note, if you want to try again you don't need to run all this. Just re-run the prompt cell one cell up and it'll generate a new image."
      ],
      "metadata": {
        "id": "ES1dGbFSQ9rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = 2\n",
        "\n",
        "prompts = [ prompts ] * num_images\n",
        "with autocast(\"cuda\"):\n",
        "    images = pipe(prompts, guidance_scale=7.5, num_inference_steps=50)[\"sample\"]\n",
        "\n",
        "media.show_images(images)\n",
        "images[0].save(\"output.jpg\")"
      ],
      "metadata": {
        "id": "AUc4QJfE-uR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}